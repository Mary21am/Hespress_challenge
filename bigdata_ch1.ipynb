{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56f48947",
   "metadata": {},
   "source": [
    "# Challenge :Sentiment Analysis System using Big Data Solution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "557fab96",
   "metadata": {},
   "source": [
    "## I. Web scraping and data ingestion to a MongoDB cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdc336cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x1bc5dfd3790>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pymongo import MongoClient\n",
    "# Define the URL to scrape\n",
    "url=\"https://www.hespress.com/%d8%b1%d8%a6%d9%8a%d8%b3-%d9%86%d8%a7%d8%af%d9%8a-%d8%a7%d9%84%d9%88%d8%af%d8%a7%d8%af-%d9%8a%d8%ad%d8%a7%d9%88%d9%84-%d8%a5%d8%a8%d9%82%d8%a7%d8%a1-%d8%b9%d8%b7%d9%8a%d8%a9-%d8%a7%d9%84%d9%84%d9%87-1121462.html/\"\n",
    "# Make a GET request to the website\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML using BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Extract the comments from the HTML\n",
    "commentaires = soup.find_all(\"div\", class_=\"commentaires\")\n",
    "\n",
    "# Connect to the MongoDB instance\n",
    "client = MongoClient(\"mongodb+srv://maryamElmouim:azera1232000@serverlessinstance0.ww03u.mongodb.net/test\")\n",
    "\n",
    "# Get the comments collection\n",
    "comments_collection = client.test.comments\n",
    "\n",
    "comments_to_insert = []\n",
    "# Iterate through the comments and extract the text of the two div elements\n",
    "for comment in commentaires:\n",
    "    for ul in comment.find_all('ul'):\n",
    "        for li in ul.find_all('li'):\n",
    "            div_container = li.find('div', {'class': 'comment-body'})\n",
    "            div1 = div_container.find('div', {'class': 'comment-head'})\n",
    "            div2 = div_container.find('div', {'class': 'comment-text'})\n",
    "            comment_obj = {\"author\": div1.text.split(\"\\n\")[2], \"text\": div2.text}\n",
    "            comments_to_insert.append(comment_obj)\n",
    "\n",
    "# insert all comments\n",
    "comments_collection.insert_many(comments_to_insert)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "777c4d0d",
   "metadata": {},
   "source": [
    "Step 2. Data ingestion into a Kafka topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664537a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer, Consumer\n",
    "import json # saving format \n",
    "\n",
    "# setting the Kafka producer\n",
    "producer = Producer({'bootstrap.servers': 'localhost:9092'})\n",
    "\n",
    "# extracting the text of the two div elements\n",
    "for comment in comments:\n",
    "    for ul in comment.find_all('ul'):\n",
    "        for li in ul.find_all('li'):\n",
    "            div_container = li.find('div', {'class': 'comment-body'})\n",
    "            div1 = div_container.find('div', {'class': 'comment-head'})\n",
    "            div2 = div_container.find('div', {'class': 'comment-text'})\n",
    "            comment_obj = {\"author\": div1.text.split(\"\\n\")[2], \"text\": div2.text}\n",
    "            comment_json = json.dumps(comment_obj)\n",
    "            producer.produce('exam', value=bytes(comment_json, 'utf-8'))\n",
    "            producer.flush()\n",
    "            print(f'Published message: {comment_json}')\n",
    "\n",
    "\n",
    "# setting the Kafka consumer\n",
    "conf = {'bootstrap.servers': 'localhost:9092',\n",
    "        'group.id': 'mygroup',\n",
    "        'auto.offset.reset': 'earliest'}\n",
    "\n",
    "consumer = Consumer(conf)\n",
    "\n",
    "consumer.subscribe(['exam'])\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    msg = consumer.poll(1.0)\n",
    "    if msg is None:\n",
    "        continue\n",
    "    if msg.error():\n",
    "        if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "            print('Reached end of topic {} [{}] at offset {}'.format(\n",
    "                msg.topic(), msg.partition(), msg.offset()))\n",
    "        else:\n",
    "            print('Error occured: {}'.format(msg.error()))\n",
    "    else:\n",
    "        comment = json.loads(msg.value())\n",
    "        print(comment)\n",
    "consumer.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0635fc02",
   "metadata": {},
   "source": [
    "\n",
    "step 3. real-time sentiment analysis using the 'NLTK' library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f5e9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "for comment in comments:\n",
    "    for ul in comment.find_all('ul'):\n",
    "        for li in ul.find_all('li'):\n",
    "            div_container = li.find('div', {'class': 'comment-body'})\n",
    "            div2 = div_container.find('div', {'class': 'comment-text'})\n",
    "            comment_text = div2.text\n",
    "            blob = TextBlob(comment_text)\n",
    "            sentiment = blob.sentiment\n",
    "            print(sentiment)\n",
    "while True:\n",
    "    msg = consumer.poll(1.0)\n",
    "    if msg is None:\n",
    "        continue\n",
    "    if msg.error():\n",
    "        if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "            print('Reached end of topic {} [{}] at offset {}'.format(\n",
    "                msg.topic(), msg.partition(), msg.offset()))\n",
    "        else:\n",
    "            print('Error occured: {}'.format(msg.error()))\n",
    "    else:\n",
    "        comment = json.loads(msg.value())\n",
    "        print(comment)\n",
    "# close the consumer after the loop\n",
    "consumer.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9dfbc996",
   "metadata": {},
   "source": [
    "## III. Apache Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "695572b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "#setting spark environement\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9913d2a7",
   "metadata": {},
   "source": [
    "<li>Batch processing </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44b19be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"BatchProcessing\").getOrCreate()\n",
    "\n",
    "# Load data from the local JSON file into a DataFrame\n",
    "df = spark.read.option(\"multiline\",\"true\").json(r\"C:\\Users\\21263\\Downloads\\comments.json\")\n",
    "\n",
    "# Specify the batch size for processing\n",
    "batch_size = 82\n",
    "\n",
    "# Get the number of records in the DataFrame\n",
    "num_records = df.count()\n",
    "print(num_records)\n",
    "# Calculate the number of batches to process\n",
    "num_batches = (num_records + batch_size - 1) // batch_size\n",
    "print(num_batches)\n",
    "\n",
    "# Loop through the batches and process the data\n",
    "for i in range(num_batches):\n",
    "    # Get the start and end index for the current batch\n",
    "    start_index = i * batch_size\n",
    "    end_index = min(start_index + batch_size, num_records)\n",
    "    \n",
    "    # Get the DataFrame for the current batch\n",
    "    batch_df = df.limit(batch_size).coalesce(1)\n",
    "    \n",
    "    \n",
    "    # Perform batch processing on the data\n",
    "    result = batch_df.rdd\\\n",
    "        .map(lambda x: (x.author, x.text))\\\n",
    "        .groupByKey()\\\n",
    "        .mapValues(list)\\\n",
    "        .collect()\n",
    "    \n",
    "    # Show result\n",
    "    for author, text in result:\n",
    "        print(f\"Author: {author}\\nText: {text}\\n\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4823766d",
   "metadata": {},
   "source": [
    "\n",
    "<li>Sentiment analysis using the library 'TextBlob' by batch processing</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e17f4627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: bernoussi \n",
      "Average sentiment: 0.03333333333333333\n",
      "\n",
      "Author: lahcen \n",
      "Average sentiment: 0.16666666666666669\n",
      "\n",
      "Author: Oujdi \n",
      "Average sentiment: 0.25\n",
      "\n",
      "Author: Ahmed faras \n",
      "Average sentiment: 0.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"BatchProcessing\").getOrCreate()\n",
    "\n",
    "# Load data from the local JSON file into a DataFrame\n",
    "df = spark.read.option(\"multiline\",\"true\").json(r\"C:\\Users\\21263\\Downloads\\comments.json\")\n",
    "\n",
    "# Perform batch processing on the data\n",
    "result = df.rdd\\\n",
    "    .map(lambda x: (x.author, x.text))\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(list)\\\n",
    "    .collect()\n",
    "\n",
    "# Perform sentiment analysis using TextBlob\n",
    "for author, text in result:\n",
    "    sentiment_scores = [TextBlob(t).sentiment.polarity for t in text]\n",
    "    average_sentiment = sum(sentiment_scores) / len(sentiment_scores)\n",
    "    if average_sentiment != 0:\n",
    "        print(f\"Author: {author}\\nAverage sentiment: {average_sentiment}\\n\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
